本篇开始介绍kendynet的设计思路.


首先介绍最base的API,这些API就是对bsd socket的一个简单封装，提供简单的异步connect/accpet/recv/send功能.

kendynet的网络接口模仿了IOCP的overlap结构，send/recv请求提供的不是一个缓冲区，而是如下所示的一个被称为io请求的结构体:

	typedef struct
	{
    	lnode      next;
		struct     iovec *iovec;
		int32_t    iovec_count;
	}st_io;

`iovec`里存放的是用户用于发送/接收的缓冲区地址和缓冲区大小,`iovec_count`是用户提供的缓冲的数量.

kendynet在底层使用的是epoll的edge触发模式,也就是当socket从不可读/写转变为可读/写的情况下才会触发.熟悉level触发模式的同学应该都知道，在level触发模式下,对可写的监听与可读监听是不一样的,因为在大多数情况下socket都处于可写状态，如果一直监听socket的可写事件则epoll_wait就会一直触发.所以在level模式下对写监听的处理一般都是将socket设置为非阻塞模式,然后直接send,如果send返回EAGAIN才添加对这个套接字的可写监听.


下面看下epoll主循环的处理:

	int32_t epoll_loop(poller_t n,int32_t ms)
	{
		assert(n);
		if(ms < 0)ms = 0;
		uint64_t sleep_ms;
		uint64_t timeout = GetSystemMs64() + (uint64_t)ms;
		uint64_t current_tick;
		uint32_t read_event = EV_IN | EPOLLRDHUP | EPOLLERR | EPOLLHUP;
		int32_t notify = 0;
		do{
	
	        if(!dlist_empty(&n->connecting))
		    {
		        //check timeout connecting
		        uint64_t l_now = GetSystemMs64();
	            dlist_check_remove(&n->connecting,check_connect_timeout,(void*)&l_now);
		    }
	        if(!is_active_empty(n))
	        {
	            struct dlist *actived = get_active_list(n);
	            n->actived_index = (n->actived_index+1)%2;
	            socket_t s;
	            while((s = (socket_t)dlist_pop(actived)) != NULL)
	            {
	                if(Process(s))
	                    putin_active(n,(struct dnode*)s);
	            }
	        }
			current_tick = GetSystemMs64();
	        if(is_active_empty(n))
				sleep_ms = timeout > current_tick ? timeout - current_tick:0;
			else
				sleep_ms = 0;
			notify = 0;
	        int32_t nfds = _epoll_wait(n->poller_fd,n->events,MAX_SOCKET,(uint32_t)sleep_ms);
			if(nfds < 0)
				return -1;
			int32_t i;
			for(i = 0 ; i < nfds ; ++i)
			{
				if(n->events[i].data.fd == n->pipe_reader)
				{
					char buf[1];
					read(n->pipe_reader,buf,1);
					notify = 1;
				}else{
					socket_t sock = (socket_t)n->events[i].data.ptr;
					if(sock)
					{
						if(sock->socket_type == CONNECT){
							process_connect(sock);
						}
						else if(sock->socket_type == LISTEN){
							process_accept(sock);
						}
						else{
							if(n->events[i].events & read_event)
								on_read_active(sock);
							if(n->events[i].events & EPOLLOUT)
								on_write_active(sock);
						}
					}
				}
			}
			current_tick = GetSystemMs64();
		}while(notify == 0 && timeout > current_tick);
		return 0;
	}


我们首先关注代码中的这个部分:

	        if(!is_active_empty(n))
	        {
	            struct dlist *actived = get_active_list(n);
	            n->actived_index = (n->actived_index+1)%2;
	            socket_t s;
	            while((s = (socket_t)dlist_pop(actived)) != NULL)
	            {
	                if(Process(s))
	                    putin_active(n,(struct dnode*)s);
	            }
	        }

这里面有个一'get_active_list'函数的调用,这里需要解释下什么是`actived list`.

一个套接口处于`actived list`需要满足两个条件,套接口处于可读/写状态,同时有相应的请求.

这里我们跳过去看下socket结构的定义:
	
	typedef struct socket_wrapper
	{
	    struct dnode       node;
	    struct refbase     ref;
	    volatile uint32_t  status;
		volatile int32_t  readable;
		volatile int32_t  writeable;
	    struct poller     *engine;
		int32_t fd;
	    struct llist      pending_send;//尚未处理的发请求
	    struct llist      pending_recv;//尚未处理的读请求
	    int8_t  socket_type;           //DATA or ACCEPTOR
	    struct sockaddr_in addr_local;
	    struct sockaddr_in addr_remote;
	    union{
		    //for data socket
	        struct{
	            void (*io_finish)(int32_t,st_io*,uint32_t err_code);
	            void (*clear_pending_io)(st_io*);
	        };
	        //for listen or Connecting socket
	        struct {
	            uint64_t timeout;
	            void *ud;
	            SOCK  sock;
	            union{
	                void (*on_accept)(SOCK,struct sockaddr_in*,void*);
	                void (*on_connect)(SOCK,struct sockaddr_in*,void*,int);
	            };
	        };
		};
	}*socket_t;


这里要注意的是下面这两个成员：

    struct llist      pending_send;//尚未处理的发请求
    struct llist      pending_recv;//尚未处理的读请求

`pending_send`和`pending_recv`链表，里面的内容就是`st_io`，代表了当前待处理的I/O请求.

也就是说一个套接口要处于`actived list`中,必须满足要么套接口可读同时`pending_recv`非空，
要么套接口可写同时`pending_send`非空.

我们在去看看`process`函数的实现:

	int32_t  Process(socket_t s)
	{	
		acquire_socket_wrapper((SOCK)s);
		_recv(s);
		_send(s);
	    int32_t read_active = s->readable && !LLIST_IS_EMPTY(&s->pending_recv);
	    int32_t write_active = s->writeable && !LLIST_IS_EMPTY(&s->pending_send);
		release_socket_wrapper((SOCK)s);
		return (read_active || write_active);
	}

先执行`_recv`和`_send`尝试完成用户的I/O请求，然后再重新计算这个socket是否还需要保留在`actived list`中.如果还需要保留在`actived list`中则在执行完`process`之后调用`putin_active(n,(struct dnode*)s);`将这个套接口重新添加到`actived list`中.

一个socket最初是不在`actived list`中的，那么它是怎么被添加到`actived list`中的呢,先看下面两个函数:

	void on_read_active(socket_t s)
	{
	    s->readable = 1;
	    if(!LLIST_IS_EMPTY(&s->pending_recv)){
	         putin_active(s->engine,(struct dnode*)s);
	    }
	}
	
	void on_write_active(socket_t s)
	{
	    s->writeable = 1;
	    if(!LLIST_IS_EMPTY(&s->pending_send)){
	        putin_active(s->engine,(struct dnode*)s);
	    }
	}

这两个函数在套接口变成可读/写状态时被回调,它们首先设置套接口的`readable`/`writeable`标记,
然后检查是否有用于I/O请求，如果有则将套接口添加到`actived list`中.

然后再看下面两个函数,它们用于投递一个读/写请求:

	int32_t Post_Recv(SOCK sock,st_io *io)
	{
		assert(io);
		socket_t s = get_socket_wrapper(sock);
	    if(!s || !test_recvable(s->status))
			return -1;
	    LLIST_PUSH_BACK(&s->pending_recv,io);
		if(s->engine && s->readable)
		{
	        putin_active(s->engine,(struct dnode*)s);
		}
		return 0;
	}
	
	int32_t Post_Send(SOCK sock,st_io *io)
	{
		assert(io);
		socket_t s = get_socket_wrapper(sock);
	    if(!s || !test_sendable(s->status))
			return -1;
	    LLIST_PUSH_BACK(&s->pending_send,io);
		if(s->engine && s->writeable)
		{
	        putin_active(s->engine,(struct dnode*)s);
		}
		return 0;
	}

它们首先将请求放到队列中,然后看套接口当前是否处于可读/写状态,如果处于则将套接口投入到`actived list`中.

未完待续```